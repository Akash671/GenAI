{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "cNlQ82YrAJ9O",
        "outputId": "e856460f-9417-4372-ab30-71b4c4966676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meaning of life is a philosophical question that has been explored by humans for centuries. There is no one definitive answer, and the meaning of life may vary depending on one's individual beliefs and values. Some common themes that emerge when discussing the meaning of life include:\n",
            "\n",
            "* **Purpose:** Many people believe that life has a purpose, whether it be to create, to learn, or to love. A sense of purpose can provide direction and meaning to one's life.\n",
            "* **Relationships:** For many people, relationships with family, friends, and loved ones provide meaning and connection. Building and maintaining strong relationships can help us feel loved and supported.\n",
            "* **Happiness:** Many people believe that the meaning of life is to be happy. Happiness can be found in many different ways, such as spending time with loved ones, pursuing our passions, or helping others.\n",
            "* **Contribution:** Some people believe that the meaning of life is to make a contribution to the world. Whether it be through our work, our relationships, or our creativity, many people find purpose in making a difference in the world.\n",
            "* **Growth:** Some people believe that the meaning of life is to grow and learn. Life is an ongoing journey of learning and self-improvement, and many people find meaning in the process of personal growth.\n",
            "\n",
            "Ultimately, the meaning of life is a personal question that each individual must explore and answer for themselves. There is no right or wrong answer, and the meaning of life may change throughout our lives as we grow and change.\n"
          ]
        }
      ],
      "source": [
        "# importing google.generativeai as genai\n",
        "import google.generativeai as genai\n",
        "\n",
        "# setting the api key\n",
        "genai.configure(api_key=\"\")\n",
        "\n",
        "# setting the text model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "# generating response\n",
        "response = model.generate_content(\"What is the meaning of life?\")\n",
        "\n",
        "# printing the response\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"Write a story about a village.\"  # Example prompt\n",
        "response = model.generate_content(\"Write a story about a village.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "VoZaSoCIRspx",
        "outputId": "52692224-bea1-4299-9c31-964f616ff148"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nestled amidst rolling hills and verdant fields, lay the quaint village of Willow Creek. Its cobblestone streets whispered tales of bygone eras, and the air carried the sweet scent of blooming wildflowers.\n",
            "\n",
            "The heart of the village was the bustling town square. Here, villagers gathered at the market to exchange wares and gossip. The town hall, its weathered walls adorned with faded paint, served as the center of community life.\n",
            "\n",
            "Morning dawned with the gentle clanging of blacksmith's hammers ringing through the air. The village blacksmith, burly and skilled, crafted tools and ornaments that were indispensable to the community. As the sun climbed higher, farmers tended to their fields, planting seeds that would sustain the village for the coming year.\n",
            "\n",
            "In the afternoons, children filled the square with their laughter and games. They chased butterflies, skipped rope, and built miniature houses out of stones. The village elder, a wise and respected woman, would sit on the bench beneath the ancient oak tree, sharing stories and imparting wisdom to the young ones.\n",
            "\n",
            "As dusk descended, the villagers gathered at the tavern for a pint and a bite to eat. The aroma of roasted meats and freshly baked bread filled the air, creating an atmosphere of camaraderie and warmth. Travelers from far-off lands regaled the villagers with tales of their adventures, expanding their horizons and fueling their imaginations.\n",
            "\n",
            "At night, the village was illuminated by the soft glow of lanterns. Crickets chirped in the fields, and a gentle breeze rustled the leaves of the trees. Stars twinkled above, casting a silvery shimmer over the sleepy village.\n",
            "\n",
            "Willow Creek was more than just a collection of buildings and streets. It was a tapestry woven with the threads of its people's lives. It was a place where traditions were honored, where neighbors helped neighbors, and where the spirit of community prevailed.\n",
            "\n",
            "Centuries passed, but the village remained unchanged. It was a sanctuary where time seemed to stand still, a testament to the enduring bonds that connected its inhabitants to their land and to one another. And so, Willow Creek continued to thrive, a timeless haven nestled amidst the changing world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FiicXKwVvM7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "q2pXT7CpVhrb",
        "outputId": "7483aeeb-fd3c-4353-ee42-cac987a7d557"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'models/gemini-pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(genai.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3OKN0KERvOqp",
        "outputId": "4046b4d3-eaac-457d-813d-43c96394fc18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}