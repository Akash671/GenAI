{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ],
      "metadata": {
        "id": "n_GAh6aClE4Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "d_model = 512  # Embedding size\n",
        "nhead = 8      # Number of heads in multi-head attention\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.1\n",
        "max_seq_length = 100  # Max length of input sequences"
      ],
      "metadata": {
        "id": "PMXE-MA3lG6Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        self.encoding.requires_grad = False  # No gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        _2i = torch.arange(0, d_model, 2)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.encoding[:seq_len, :].to(x.device)"
      ],
      "metadata": {
        "id": "F8dooe5tlOSP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nhead = nhead\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        def transform(x):\n",
        "            x = x.view(batch_size, -1, self.nhead, self.d_model // self.nhead)\n",
        "            return x.transpose(1, 2)\n",
        "\n",
        "        query = transform(self.query(query))\n",
        "        key = transform(self.key(key))\n",
        "        value = transform(self.value(value))\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention = torch.nn.functional.softmax(scores, dim=-1)\n",
        "\n",
        "        x = torch.matmul(attention, value)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.out(x)"
      ],
      "metadata": {
        "id": "pv1rvcYFlSQX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dim_feedforward, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "I2TnCGXBlVon"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
        "        self.ff = FeedForward(d_model, dim_feedforward, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, src_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.ff(src)\n",
        "        src = src + self.dropout2(src2)\n",
        "        return self.norm2(src)"
      ],
      "metadata": {
        "id": "QgEuRMItlYz4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
        "        self.multihead_attn = MultiHeadAttention(d_model, nhead)\n",
        "        self.ff = FeedForward(d_model, dim_feedforward, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, memory_mask)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.ff(tgt)\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return self.norm3(tgt)"
      ],
      "metadata": {
        "id": "mkfd2xipluXY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(d_model)\n",
        "        src = self.pos_encoding(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return self.norm(src)\n"
      ],
      "metadata": {
        "id": "Tcb2K0FvmARX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt = self.embedding(tgt) * math.sqrt(d_model)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
        "        return self.fc_out(self.norm(tgt))"
      ],
      "metadata": {
        "id": "AJOF5ov-mFT_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "        self.decoder = Decoder(output_dim, d_model, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "BSpjkVo9mH-A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH0FyIBElCf1",
        "outputId": "be075a2c-0ca1-4a3f-dafd-6475b6d1e05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 10000])\n",
            "Epoch 1, Loss: 9.400156021118164\n",
            "Epoch 2, Loss: 5.683928489685059\n",
            "Epoch 3, Loss: 5.805520057678223\n",
            "Epoch 4, Loss: 4.35227108001709\n",
            "Epoch 5, Loss: 4.571855545043945\n",
            "Epoch 6, Loss: 2.846435070037842\n",
            "Epoch 7, Loss: 5.040071487426758\n",
            "Epoch 8, Loss: 3.4016904830932617\n",
            "Epoch 9, Loss: 2.0712385177612305\n",
            "Epoch 10, Loss: 0.8312703967094421\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example parameters (can be adjusted)\n",
        "    input_dim = 10000  # Vocabulary size of the source language\n",
        "    output_dim = 10000  # Vocabulary size of the target language\n",
        "\n",
        "    model = Transformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "    # Dummy data (batch_size=2, sequence_length=10)\n",
        "    src = torch.randint(0, input_dim, (2, 10))  # Source sentence\n",
        "    tgt = torch.randint(0, output_dim, (2, 10))  # Target sentence\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src, tgt)\n",
        "\n",
        "    # Output shape: (batch_size, tgt_sequence_length, output_dim)\n",
        "    print(output.shape)\n",
        "\n",
        "    # Defining the optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Dummy training loop\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, tgt[:, :-1])  # Predict the next token\n",
        "        loss = criterion(output.view(-1, output_dim), tgt[:, 1:].reshape(-1))  # Compare with the actual token\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Assuming the Transformer class and other related classes are already defined above.\n",
        "\n",
        "# A helper function to create the target mask\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Function to perform inference and predict the next word\n",
        "def predict_next_word(model, src_sentence, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    src = torch.tensor(src_sentence).unsqueeze(0)  # Add batch dimension\n",
        "    tgt = torch.zeros(1, 1).long()  # Start token for target sentence\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        output = model(src, tgt, tgt_mask=tgt_mask)\n",
        "        next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Get the highest probability word\n",
        "\n",
        "        tgt = torch.cat((tgt, next_token), dim=1)  # Append the predicted word to the target sequence\n",
        "\n",
        "        if next_token.item() == 3:  # Assuming 3 is the <eos> token\n",
        "            break\n",
        "\n",
        "    return tgt.squeeze().tolist()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example parameters (can be adjusted)\n",
        "    input_dim = 10000  # Vocabulary size of the source language\n",
        "    output_dim = 10000  # Vocabulary size of the target language\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = Transformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "    # Dummy data (for example purposes, usually it would be actual sentences)\n",
        "    src_sentence = [1, 5, 6, 7, 2]  # Example tokenized source sentence (1=start, 2=end tokens)\n",
        "\n",
        "    # Assume the model is already trained, or you can load pre-trained weights here.\n",
        "\n",
        "    # Perform inference\n",
        "    predicted_sentence = predict_next_word(model, src_sentence)\n",
        "\n",
        "    print(f\"Predicted tokens: {predicted_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnTFJe41mlLG",
        "outputId": "be55c103-c686-41f0-9d7e-3aa5cb42535a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted tokens: [0, 7708, 3685, 9907, 2743, 6693, 3357, 9907, 2743, 6693, 3357, 9907, 416, 1607, 1172, 937, 2241, 1939, 3466, 1155, 219]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Assuming the Transformer class and other related classes are already defined above.\n",
        "\n",
        "# A helper function to create the target mask\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Function to perform inference and predict the next word\n",
        "def predict_next_word(model, src_sentence, vocab, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    src = torch.tensor(src_sentence).unsqueeze(0)  # Add batch dimension\n",
        "    tgt = torch.zeros(1, 1).long()  # Start token for target sentence\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        output = model(src, tgt, tgt_mask=tgt_mask)\n",
        "        next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Get the highest probability word\n",
        "\n",
        "        tgt = torch.cat((tgt, next_token), dim=1)  # Append the predicted word to the target sequence\n",
        "\n",
        "        if next_token.item() == vocab['<eos>']:  # Assuming <eos> is the end token\n",
        "            break\n",
        "\n",
        "    # Convert tokens to words\n",
        "    predicted_tokens = tgt.squeeze().tolist()\n",
        "    predicted_words = [vocab.get(token, \"<unk>\") for token in predicted_tokens]\n",
        "\n",
        "    return \" \".join(predicted_words)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example parameters (can be adjusted)\n",
        "    input_dim = 10000  # Vocabulary size of the source language\n",
        "    output_dim = 10000  # Vocabulary size of the target language\n",
        "\n",
        "    # Example vocabulary (you should replace this with your actual vocabulary)\n",
        "    vocab = {\n",
        "        1: \"<start>\",\n",
        "        2: \"<eos>\",\n",
        "        3: \"The\",\n",
        "        4: \"dog\",\n",
        "        5: \"barked\",\n",
        "        6: \"at\",\n",
        "        7: \"the\",\n",
        "        8: \"cat\",\n",
        "        # ... (other tokens in your vocabulary)\n",
        "    }\n",
        "    # Reverse the vocabulary (token to word mapping)\n",
        "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = Transformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "    # Dummy data (for example purposes, usually it would be actual sentences)\n",
        "    src_sentence = [reverse_vocab[\"<start>\"], reverse_vocab[\"The\"], reverse_vocab[\"dog\"], reverse_vocab[\"barked\"], reverse_vocab[\"<eos>\"]]  # Example tokenized source sentence\n",
        "\n",
        "    # Assume the model is already trained, or you can load pre-trained weights here.\n",
        "\n",
        "    # Perform inference\n",
        "    predicted_sentence = predict_next_word(model, src_sentence, reverse_vocab)\n",
        "\n",
        "    print(f\"Predicted sentence: {predicted_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5gK6xgDnAwn",
        "outputId": "39c783ae-8db4-4ec2-d5e5-7caa8cc64e1c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentence: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "\n",
        "# Assume the Transformer class, PositionalEncoding, MultiHeadAttention, etc., are already defined as per the previous code.\n",
        "\n",
        "# Define a simple dataset for sequence-to-sequence tasks\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences, src_vocab, tgt_vocab):\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure tokens are within the valid range\n",
        "        src = [self.src_vocab.get(token, self.src_vocab[\"<unk>\"]) for token in self.source_sentences[idx]]\n",
        "        tgt = [self.tgt_vocab.get(token, self.tgt_vocab[\"<unk>\"]) for token in self.target_sentences[idx]]\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "# Custom collate function to handle variable length sequences\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_item, tgt_item in batch:\n",
        "        src_batch.append(src_item)\n",
        "        tgt_batch.append(tgt_item)\n",
        "    # Pad sequences to the same length\n",
        "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0) # Assuming 0 is the padding token\n",
        "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# A helper function to create the target mask\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, data_loader, optimizer, criterion, num_epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in data_loader:\n",
        "            src = src.to(device)\n",
        "            tgt_input = tgt[:, :-1].to(device)  # Input for the model\n",
        "            tgt_output = tgt[:, 1:].to(device)  # Expected output\n",
        "\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
        "\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), tgt_output.contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example vocabularies (as provided above)\n",
        "    src_vocab = {\n",
        "        \"<start>\": 1, \"<eos>\": 2, \"<unk>\": 3,\n",
        "        \"The\": 4, \"dog\": 5, \"barked\": 6, \"at\": 7, \"the\": 8, \"cat\": 9,\n",
        "        \"A\": 10, \"man\": 11, \"is\": 12, \"walking\": 13, \"in\": 14, \"park\": 15,\n",
        "        \"Hello\": 16, \"world\": 17\n",
        "    }\n",
        "    tgt_vocab = {\n",
        "        \"<start>\": 1, \"<eos>\": 2, \"<unk>\": 3,\n",
        "        \"Le\": 4, \"chien\": 5, \"aboie\": 6, \"à\": 7, \"le\": 8, \"chat\": 9,\n",
        "        \"Un\": 10, \"homme\": 11, \"marche\": 12, \"dans\": 13, \"le\": 14, \"parc\": 15,\n",
        "        \"Bonjour\": 16, \"le\": 17, \"monde\": 18\n",
        "    }\n",
        "\n",
        "    # Example sentences (as provided above)\n",
        "    source_sentences = [\n",
        "        [\"<start>\", \"The\", \"dog\", \"barked\", \"at\", \"the\", \"cat\", \"<eos>\"],\n",
        "        [\"<start>\", \"A\", \"man\", \"is\", \"walking\", \"in\", \"the\", \"park\", \"<eos>\"],\n",
        "        [\"<start>\", \"Hello\", \"world\", \"<eos>\"],\n",
        "    ]\n",
        "    target_sentences = [\n",
        "        [\"<start>\", \"Le\", \"chien\", \"aboie\", \"à\", \"le\", \"chat\", \"<eos>\"],\n",
        "        [\"<start>\", \"Un\", \"homme\", \"marche\", \"dans\", \"le\", \"parc\", \"<eos>\"],\n",
        "        [\"<start>\", \"Bonjour\", \"le\", \"monde\", \"<eos>\"],\n",
        "    ]\n",
        "\n",
        "    # Prepare the dataset and data loader\n",
        "    dataset = Seq2SeqDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)\n",
        "    # Use the custom collate function\n",
        "    data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "    # Prepare the dataset and data loader\n",
        "    #dataset = Seq2SeqDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)\n",
        "    #data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "    # Define model parameters\n",
        "    input_dim = len(src_vocab)  # Vocabulary size of the source language\n",
        "    output_dim = len(tgt_vocab)  # Vocabulary size of the target language\n",
        "    # Define model parameters\n",
        "    #input_dim = len(src_vocab) + 1  # Vocabulary size of the source language\n",
        "    #output_dim = len(tgt_vocab) + 1  # Vocabulary size of the target language\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #model = Transformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<unk>\"])  # Ignore the <unk> token during loss calculation\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, data_loader, optimizer, criterion, num_epochs=10)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"transformer_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Rk3O6LntUBk2",
        "outputId": "6148e8a0-597f-40b1-ebb9-e7eeae3094fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6705ca616829>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6705ca616829>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtgt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtgt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference example\n",
        "reverse_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "src_sentence = [src_vocab[\"<start>\"], src_vocab[\"The\"], src_vocab[\"dog\"], src_vocab[\"barked\"], src_vocab[\"<eos>\"]]  # Example source sentence\n",
        "# Pass an integer for max_length (e.g., 20)\n",
        "predicted_sentence = predict_next_word(model, src_sentence, tgt_vocab, max_length=20) # Pass tgt_vocab instead of reverse_vocab\n",
        "\n",
        "print(f\"Predicted sentence: {predicted_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly-q-tpqXB64",
        "outputId": "b175c112-8017-43f1-fb23-ee3d0f505e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentence: <unk> <unk>\n"
          ]
        }
      ]
    }
  ]
}