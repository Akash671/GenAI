{

  "nbformat": 4,

  "nbformat_minor": 0,

  "metadata": {

    "colab": {

      "provenance": []

    },

    "kernelspec": {

      "name": "python3",

      "display_name": "Python 3"

    },

    "language_info": {

      "name": "python"

    }

  },

  "cells": [

    {

      "cell_type": "markdown",

      "source": [

        "#Text Preprocessing\n",

        "##Text cleaning: removing noise, special characters, and stop words\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "##Introduction to regular expressions for text preprocessing\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "##Hands-on exercise: Preprocessing text data using Python libraries (e.g., NLTK or spaCy)"

      ],

      "metadata": {

        "id": "HHbImldrFnjO"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "In this notebook, we first import the necessary libraries such as nltk and re for text preprocessing. We then download the stopwords corpus from NLTK using nltk.download('stopwords').\n",

        "\n",

        "Next, we define the clean_text function that performs text cleaning. It converts the text to lowercase, removes special characters using regular expressions, and removes stopwords using NLTK's stopwords corpus.\n",

        "\n",

        "*italicized text*\n",

        "We provide a sample text data and apply the clean_text function to obtain the cleaned text. Finally, we print both the original and cleaned text."

      ],

      "metadata": {

        "id": "_jly7UIzNZBR"

      }

    },

    {

      "cell_type": "code",

      "execution_count": 1,

      "metadata": {

        "id": "Opfe9xG5tb-t"

      },

      "outputs": [],

      "source": [

        "#Preprocessing text data using Python libraries\n",

        "\n",

        "## Importing necessary libraries\n",

        "import nltk\n",

        "from nltk.corpus import stopwords\n",

        "import re\n"

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "## Downloading stopwords from NLTK\n",

        "nltk.download('stopwords')"

      ],

      "metadata": {

        "id": "ozSfHWNJGH0I"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Sample text data\n",

        "text_data = \"This is an example sentence! It contains special characters like @#$% and stopwords such as 'the' and 'is'.\""

      ],

      "metadata": {

        "id": "mHpwH7_lGMYH"

      },

      "execution_count": 3,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Text cleaning: Removing noise, special characters, and stop words\n",

        "def clean_text1(text):\n",

        "    # Convert text to lowercase\n",

        "    text = text.lower()\n",

        "\n",

        "    # Remove special characters using regular expressions\n",

        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",

        "    print(text)\n",

        "\n",

        "    # Remove stopwords\n",

        "    stop_words = set(stopwords.words('english'))\n",

        "    tokens = text.split()\n",

        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",

        "    text = ' '.join(filtered_tokens)\n",

        "\n",

        "    return text"

      ],

      "metadata": {

        "id": "n0f_YzIyGS8Q"

      },

      "execution_count": 7,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Cleaning the text data\n",

        "cleaned_text = clean_text1(text_data)\n"

      ],

      "metadata": {

        "id": "Kwq9FPIbGYyU"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Printing the cleaned text\n",

        "print(\"Original Text:\", text_data)\n",

        "print(\"Cleaned Text:\", cleaned_text)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "w8W9NwtfGdmJ",

        "outputId": "7f89ceec-34d5-4d0c-bf27-d745c3b7a45c"

      },

      "execution_count": 6,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Original Text: This is an example sentence! It contains special characters like @#$% and stopwords such as 'the' and 'is'.\n",

            "Cleaned Text: example sentence contains special characters like stopwords\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\n",

        "\n",

        "https://spacy.io/"

      ],

      "metadata": {

        "id": "y9AXn7ZySXWd"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Preprocessing text data using Python libraries\n",

        "\n",

        "## Importing necessary libraries\n",

        "import spacy\n"

      ],

      "metadata": {

        "id": "qsJIDFPpPFRJ"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Loading the spaCy English model\n",

        "nlp = spacy.load('en_core_web_sm')"

      ],

      "metadata": {

        "id": "Es_COikSPM0U"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Sample text data\n",

        "text_data = \"This is an example sentence! It contains special characters like @#$% and stopwords such as 'the' and 'is'.\""

      ],

      "metadata": {

        "id": "Gu7S0l9tPSqY"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Text cleaning: Removing noise, special characters, and stop words\n",

        "def clean_text2(text):\n",

        "    # Convert text to spaCy Doc\n",

        "    doc = nlp(text)\n",

        "\n",

        "    # Remove special characters using regular expressions, stopwords, and whitespace\n",

        "    cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token.text) for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",

        "\n",

        "    # Join the cleaned tokens back into a single string\n",

        "    cleaned_text = ' '.join(cleaned_tokens)\n",

        "\n",

        "    return cleaned_text"

      ],

      "metadata": {

        "id": "vS1Tm6tcPW2F"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Cleaning the text data\n",

        "cleaned_text = clean_text2(text_data)"

      ],

      "metadata": {

        "id": "aYh4tqK-PkPR"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "## Printing the cleaned text\n",

        "print(\"Original Text:\", text_data)\n",

        "print(\"Cleaned Text:\", cleaned_text)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "lK2IFkLbPoxk",

        "outputId": "f265241f-579d-4842-943e-6bef7fd2cc3c"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Original Text: This is an example sentence! It contains special characters like @#$% and stopwords such as 'the' and 'is'.\n",

            "Cleaned Text: example sentence contains special characters like  stopwords\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Regular expressions (regex) are powerful tools for text pre-processing and cleaning tasks. Here are some examples of how regular expressions can be used:\n",

        "\n",

        "Removing punctuation: Regular expressions can be used to remove punctuation marks from text. For example, you can use the pattern \\p{P} to match any punctuation character and replace it with an empty string.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Removing special characters: You can use regular expressions to remove specific special characters from text. For example, to remove all non-alphanumeric characters except spaces, you can use the pattern [^a-zA-Z0-9\\s] and replace it with an empty string.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Normalizing whitespace: Regular expressions can help in normalizing whitespace by replacing multiple consecutive spaces or tabs with a single space. For example, the pattern \\s+ can be used to match one or more whitespace characters, and you can replace them with a single space.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Removing URLs or email addresses: If you want to remove URLs or email addresses from text, you can use regular expressions to match and replace them. There are various patterns available for this purpose, depending on the complexity of the URLs or email addresses you want to handle.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Extracting mentions or hashtags: Regular expressions can be used to extract mentions or hashtags from text, commonly found in social media data. For example, to extract all mentions in a tweet, you can use the pattern @(\\w+) to match the '@' symbol followed by one or more word characters.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Removing HTML tags: Regular expressions can help remove HTML tags from text. For instance, the pattern <[^>]+> can be used to match any HTML tag and replace it with an empty string.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Tokenization: Regular expressions can assist in splitting text into tokens based on specific patterns. For example, you can split a sentence into words by using the pattern \\b\\w+\\b, which matches any word character surrounded by word boundaries.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Data cleaning and formatting: Regular expressions can be used to clean and format specific data formats, such as phone numbers, dates, or postal codes. You can define patterns to match the desired format and then manipulate or extract the relevant information."

      ],

      "metadata": {

        "id": "lSnCuMmQTKkU"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Removing punctuation:"

      ],

      "metadata": {

        "id": "DYt8PmEoTyDa"

      }

    },

    {

     "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"Hello, world!\"\n",

        "clean_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",

        "print(clean_text)  # Output: Hello world\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "D69JTVUIT0Df",

        "outputId": "2f6b3a1c-50f6-4e45-a576-62ce265acc84"

      },

      "execution_count": 9,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Hello world\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Removing special characters:\n"

      ],

      "metadata": {

        "id": "fd_ZeeF1UIKt"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"Hello@#$ world!\"\n",

        "clean_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",

        "print(clean_text)  # Output: Hello world\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "kmRoI0mvV4a4",

        "outputId": "29614ae7-7908-4721-9590-5d1537b59197"

      },

      "execution_count": 10,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Hello world\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Normalizing whitespace:\n"

      ],

      "metadata": {

        "id": "a4RFiu0wV80U"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"Hello    world!\"\n",

        "clean_text = re.sub(r\"\\s+\", \" \", text)\n",

        "print(clean_text)  # Output: Hello world!\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "ruS_DRSwWAEA",

        "outputId": "7d5fa0d1-8c8f-4576-956a-4be3bc009dcd"

      },

      "execution_count": 11,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Hello world!\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Removing URLs or email addresses:\n"

      ],

      "metadata": {

        "id": "bZ-6kLFPWISA"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"Visit my website at https://example.com or email me at info@example.com\"\n",

        "clean_text = re.sub(r\"https?://\\S+|[\\w.-]+@[\\w.-]+\", \"\", text)\n",

        "print(clean_text)  # Output: Visit my website at  or email me at\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "ujiRjiFdWMhs",

        "outputId": "7013de6f-bb04-4238-b497-f2be10dea022"

      },

      "execution_count": 14,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Visit my website at  or email me at \n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Extracting mentions or hashtags:\n"

      ],

      "metadata": {

        "id": "ezifYVBtWQhi"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"This is a tweet mentioning @username and using #hashtag\"\n",

        "mentions = re.findall(r\"@\\w+\", text)\n",

        "hashtags = re.findall(r\"#\\w+\", text)\n",

        "\n",

        "print(mentions)  # Output: ['@username']\n",

        "print(hashtags)  # Output: ['#hashtag']\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "k1tNPx4PWT_O",

        "outputId": "2808acea-8580-40fb-df65-aa1895b3dfff"

      },

      "execution_count": 13,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "['@username']\n",

            "['#hashtag']\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Removing HTML tags:\n"

      ],

      "metadata": {

        "id": "bJuvvP8fWXJZ"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"<p>This is an example <b>HTML</b> text.</p>\"\n",

        "clean_text = re.sub(r\"<[^>]+>\", \"\", text)\n",

        "print(clean_text)  # Output: This is an example HTML text.\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "zMvxPyAXWgH5",

        "outputId": "47bd3aa1-2c54-4b57-c8ea-68a8b00c2ede"

      },

      "execution_count": 16,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "This is an example HTML text.\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Tokenization:\n"

      ],

      "metadata": {

        "id": "tPmJ5P9tWqp7"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"This is a sample sentence.\"\n",

        "tokens = re.findall(r\"\\b\\w+\\b\", text)\n",

        "print(tokens)  # Output: ['This', 'is', 'a', 'sample', 'sentence']\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "UkvfHAMzWtwK",

        "outputId": "5ca826a7-abfd-46d9-e451-caa8da26de5e"

      },

      "execution_count": 17,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "['This', 'is', 'a', 'sample', 'sentence']\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Data cleaning and formatting:\n"

      ],

      "metadata": {

        "id": "SHb_Tw2dWw6-"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import re\n",

        "\n",

        "text = \"Sample text with dates: 10/05/2023, 25/12/2022, and 05/09/2021.\"\n",

        "\n",

        "pattern = r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\"\n",

        "matches = re.findall(pattern, text)\n",

        "\n",

        "print(matches)\n",

        "\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "EI0rAmVcW1Mc",

        "outputId": "ce9a255a-5437-4d6a-fd36-1da48eee2e9e"

      },

      "execution_count": 18,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "['10/05/2023', '25/12/2022', '05/09/2021']\n"

          ]

        }

      ]

    }

  ]

}

 

