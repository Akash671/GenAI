{

  "nbformat": 4,

  "nbformat_minor": 0,

  "metadata": {

    "colab": {

      "provenance": []

    },

    "kernelspec": {

      "name": "python3",

      "display_name": "Python 3"

    },

    "language_info": {

      "name": "python"

    }

  },

  "cells": [

    {

      "cell_type": "markdown",

      "source": [

        "#Basics of language modeling and its applications\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "##Introduction to recurrent neural networks (RNNs) for language modeling\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "Hands-on exercise: Building a simple language model using RNNs in Python (e.g., using TensorFlow or PyTorch)"

      ],

      "metadata": {

        "id": "_pOEe534Ft_L"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "https://www.youtube.com/watch?v=lDkEC7H88_A\n",

        "\n",

        "\n"

      ],

      "metadata": {

        "id": "Gfnlj9QFKDXZ"

      }

    },

    {

      "cell_type": "code",

      "execution_count": null,

      "metadata": {

        "id": "NeTYDkpQtxEu"

      },

      "outputs": [],

      "source": [

        "# Import the necessary libraries\n",

        "import numpy as np\n",

        "import tensorflow as tf"

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Define the text corpus\n",

        "text = \"\"\"\n",

        "The quick brown fox jumps over the lazy dog.\n",

        "\"\"\"\n",

        "\n",

        "# Preprocess the text\n",

        "corpus = text.lower().split()\n",

        "unique_words = sorted(set(corpus))\n",

        "word_to_int = {word: i for i, word in enumerate(unique_words)}\n",

        "int_to_word = {i: word for i, word in enumerate(unique_words)}\n",

        "vocab_size = len(unique_words)\n",

        "\n",

        "print(word_to_int)\n",

        "print(int_to_word)\n",

        "print(unique_words)\n",

        "print(\"vocab size: \",vocab_size)\n",

        "print(\"corpus len: \",len(corpus))"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "CD9KH2D4Guyx",

        "outputId": "273182a5-9399-4ec5-b026-6a99a98219a2"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "{'brown': 0, 'dog.': 1, 'fox': 2, 'jumps': 3, 'lazy': 4, 'over': 5, 'quick': 6, 'the': 7}\n",

            "{0: 'brown', 1: 'dog.', 2: 'fox', 3: 'jumps', 4: 'lazy', 5: 'over', 6: 'quick', 7: 'the'}\n",

            "['brown', 'dog.', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the']\n",

            "vocab size:  8\n",

            "corpus len:  9\n"

          ]

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Generate training data\n",

        "sequence_length = 5\n",

        "input_sequences = []\n",

        "output_labels = []\n",

        "for i in range(len(corpus) - sequence_length):\n",

        "    sequence = corpus[i:i+sequence_length]\n",

        "    label = corpus[i+sequence_length]\n",

        "    input_sequences.append([word_to_int[word] for word in sequence])\n",

        "    output_labels.append(word_to_int[label])\n",

        "\n",

        "print(input_sequences)\n",

        "print(output_labels)\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "E3TJfwjXG0GU",

        "outputId": "756129d4-aeff-45e5-b43a-1c0dbd8e7764"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "[[7, 6, 0, 2, 3], [6, 0, 2, 3, 5], [0, 2, 3, 5, 7], [2, 3, 5, 7, 4]]\n",

            "[5, 7, 4, 1]\n"

          ]

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Convert the training data to numpy arrays\n",

        "input_sequences = np.array(input_sequences)\n",

        "output_labels = np.array(output_labels)\n",

        "\n",

        "print(input_sequences)\n",

        "print(output_labels)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "RS4otK0sG34n",

        "outputId": "1e7264ef-bef7-49f4-f895-e8c3c16e35c1"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "[[7 6 0 2 3]\n",

            " [6 0 2 3 5]\n",

            " [0 2 3 5 7]\n",

            " [2 3 5 7 4]]\n",

            "[5 7 4 1]\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "#A neural network model using the Keras API from TensorFlow\n",

        "\n",

        "##tf.keras.Sequential: This is the basic model class in Keras that allows you to build a linear stack of layers. Each layer in the model is added sequentially.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "tf.keras.layers.Embedding: This layer represents an embedding layer. It is commonly used in natural language processing tasks to convert integer-encoded input sequences into dense vectors of fixed size. The parameters used in this layer are:\n",

        "\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "vocab_size: The size of the vocabulary, which represents the total number of unique words in the input.\n",

        "10: The output dimension of the embedding layer, which represents the size of the dense embedding vector for each word.\n",

        "input_length=sequence_length: The length of the input sequences, which determines the number of time steps in the recurrent neural network (RNN).\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "tf.keras.layers.SimpleRNN: This layer represents a simple recurrent neural network (RNN). It processes the input sequence step by step and maintains an internal state that captures information about the past steps. The parameters used in this layer are:\n",

        "\n",

        "32: The number of units (or hidden neurons) in the RNN. This determines the dimensionality of the output of the RNN layer.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "tf.keras.layers.Dense: This layer represents a fully connected (dense) layer.\n",

        "It is used for mapping the output of the previous layer to the desired output shape. The parameters used in this layer are:\n",

        "\n",

        "vocab_size: The number of units (or neurons) in this layer, which corresponds to the size of the output vocabulary.\n",

        "activation='softmax': The activation function applied to the layer's outputs. In this case, the softmax activation function is used, which normalizes the output values into a probability distribution over the output vocabulary.\n",

        "\n",

        "---\n",

        "\n",

        "This allows the model to predict the probability of each word in the vocabulary."

      ],

      "metadata": {

        "id": "FtLvIl41Nf8Q"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "The model takes an input sequence of integer-encoded words, applies an embedding layer to convert them into dense vectors, processes them using a simple RNN layer, and finally maps the output to a probability distribution over the vocabulary using a dense layer with softmax activation. The model is trained to minimize the difference between its predicted probabilities and the true probabilities of the target words."

      ],

      "metadata": {

        "id": "VA73-3GMOo_P"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Create the RNN language model\n",

        "model = tf.keras.Sequential([\n",

        "    tf.keras.layers.Embedding(vocab_size, 12, input_length=sequence_length),\n",

        "    tf.keras.layers.SimpleRNN(32),\n",

        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",

        "])"

      ],

      "metadata": {

        "id": "UX7gLjIUHC3_"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "markdown",

      "source": [

        "The code snippet model.compile(loss='sparse_categorical_crossentropy', optimizer='adam') is used to configure the training process for the neural network model. Let's understand the parameters used in this function:\n",

        "\n",

        "loss='sparse_categorical_crossentropy': The loss parameter specifies the loss function that will be used to measure the discrepancy between the predicted output of the model and the true output during training. In this case, 'sparse_categorical_crossentropy' is used as the loss function. This loss function is suitable for multi-class classification problems where the target labels are integers (sparse labels), not one-hot encoded. It calculates the cross-entropy loss between the true labels and the predicted probabilities.\n",

        "\n",

        "optimizer='adam': The optimizer parameter specifies the optimization algorithm that will be used to update the weights of the neural network during training in order to minimize the loss function. In this case, 'adam' is used as the optimizer. Adam is an optimization algorithm that combines the benefits of two other popular algorithms, AdaGrad and RMSProp. It adapts the learning rate during training and performs well on a wide range of problems.\n",

        "\n",

        "By calling model.compile(loss='sparse_categorical_crossentropy', optimizer='adam'), you are configuring the model to use the sparse categorical cross-entropy loss function and the Adam optimizer. Once the model is compiled, it is ready to be trained on a specific dataset using the model.fit() function."

      ],

      "metadata": {

        "id": "vgzP-yr_PHsF"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Compile the model\n",

        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"

      ],

      "metadata": {

        "id": "QB983NSVHGvj"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "markdown",

      "source": [

        "The model.fit() function is a method in many deep learning frameworks, such as TensorFlow or Keras, that is used to train a machine learning model. It is commonly used to iteratively optimize the parameters of the model based on input data and corresponding labels.\n",

        "\n",

        "The function takes several arguments:\n",

        "\n",

        "input_sequences: This is the input data or feature set that will be used to train the model. It should be organized as a matrix or tensor, where each row represents an individual sample or observation, and each column represents a specific feature or input dimension.\n",

        "\n",

        "output_labels: These are the corresponding labels or target values that are associated with the input data. The labels represent the desired outputs or the ground truth for the given inputs. The format of the labels depends on the problem at hand, such as classification or regression.\n",

        "\n",

        "epochs: This parameter specifies the number of times the model will iterate over the entire dataset during the training process. An epoch is defined as a complete pass through the training data. Increasing the number of epochs allows the model to potentially learn more from the data, but it can also lead to overfitting if the model starts to memorize the training examples.\n",

        "\n",

        "verbose: This parameter controls the verbosity of the training process. By default, it is set to 1, which means that progress updates and logs will be displayed during training. If set to 0, the training will be performed silently without any output.\n",

        "\n",

        "During the training process, the model will undergo an iterative optimization algorithm, such as gradient descent, to adjust its internal parameters based on the input data and the associated labels. The goal is to minimize a predefined loss function, which measures the discrepancy between the model's predicted outputs and the true labels. The specific optimization algorithm and loss function used depend on the type of model and the problem being solved.\n",

        "\n",

        "After the model.fit() function completes, the trained model will have adjusted its parameters to better fit the provided data, hopefully improving its ability to make accurate predictions or classifications on new, unseen data"

      ],

      "metadata": {

        "id": "jWHbxS65PjFX"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Train the model\n",

        "model.fit(input_sequences, output_labels, epochs=1000, verbose=0)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "iVgKcGdOHKOH",

        "outputId": "59d8884e-a5dd-42da-951e-88446fb3a906"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "execute_result",

          "data": {

            "text/plain": [

              "<keras.callbacks.History at 0x7f57350a81c0>"

            ]

          },

          "metadata": {},

          "execution_count": 41

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Generate new text\n",

        "seed_text = 'The fox jumps over the'\n",

        "next_words = 10\n",

        "for _ in range(next_words):\n",

        "    seed_sequence = [word_to_int[word] for word in seed_text.lower().split()[-sequence_length:]]\n",

        "    predicted_probs = model.predict(np.array([seed_sequence]))\n",

        "    predicted_index = np.argmax(predicted_probs)\n",

        "    predicted_word = int_to_word[predicted_index]\n",

        "    seed_text += ' ' + predicted_word\n",

        "\n",

        "print(seed_text)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "dDgfm1x2QdLB",

        "outputId": "86d0de8b-18a2-417d-d6f8-24114838a4d6"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "1/1 [==============================] - 0s 35ms/step\n",

            "1/1 [==============================] - 0s 39ms/step\n",

            "1/1 [==============================] - 0s 29ms/step\n",

            "1/1 [==============================] - 0s 25ms/step\n",

            "1/1 [==============================] - 0s 25ms/step\n",

            "1/1 [==============================] - 0s 24ms/step\n",

            "1/1 [==============================] - 0s 25ms/step\n",

            "1/1 [==============================] - 0s 27ms/step\n",

            "1/1 [==============================] - 0s 26ms/step\n",

            "1/1 [==============================] - 0s 24ms/step\n",

            "The fox jumps over the over dog. the lazy over lazy lazy over the lazy\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "# Exercise 1: Change the sequence length and experiment with different values (e.g., 3, 7, 10). How does it affect the quality and coherence of the generated text?\n",

        "# Solution:\n",

        "'''\n",

        "# Example with sequence length of 3\n",

        "sequence_length = 3\n",

        "\n",

        "# Example with sequence length of 7\n",

        "sequence_length = 7\n",

        "\n",

        "# Example with sequence length of 10\n",

        "sequence_length = 10\n",

        "'''"

      ],

      "metadata": {

        "id": "-FdbltT--El3"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "# Exercise 2: Try using a different text corpus of your choice. You can use a longer text to generate more meaningful results. Observe how the choice of corpus affects the output.\n",

        "# Solution:\n",

        "'''\n",

        "# Define your own text corpus\n",

        "text = \"Your text corpus goes here.\"\n",

        "\n",

        "# Preprocess the text and generate training data\n",

        "# ...\n",

        "'''"

      ],

      "metadata": {

        "id": "aH450HtA-Jel"

      }

    },

    {

      "cell_type": "markdown",

      "source": [],

      "metadata": {

        "id": "ZrQXyw7H-uvj"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "# Exercise 3: Experiment with different hyperparameters, such as embedding size, number of hidden units, and optimizer. Observe the impact of these changes on the training time and quality of generated text.\n",

        "# Solution:\n",

        "'''\n",

        "# Example with embedding size of 20\n",

        "model = tf.keras.Sequential([\n",

        "    tf.keras.layers.Embedding(vocab_size, 20, input_length=sequence_length),\n",

        "    tf.keras.layers.SimpleRNN(32),\n",

        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",

        "])\n",

        "\n",

        "# Example with 64 hidden units\n",

        "model = tf.keras.Sequential"

      ],

      "metadata": {

        "id": "_ZZ39-3d-llQ"

      }

    },

    {

      "cell_type": "markdown",

      "source": [],

      "metadata": {

        "id": "EMN3wiVe-76J"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Exercise 4: Create the RNN language model with stacked RNN layers\n",

        "model = tf.keras.Sequential([\n",

        "    tf.keras.layers.Embedding(vocab_size, 10, input_length=sequence_length),\n",

        "    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",

        "    tf.keras.layers.SimpleRNN(32),\n",

        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",

        "])"

      ],

      "metadata": {

        "id": "rw5TZHlO_Mt2"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "# Exercise 5: Create the RNN language model with bidirectional RNN\n",

        "model = tf.keras.Sequential([\n",

        "    tf.keras.layers.Embedding(vocab_size, 10, input_length=sequence_length),\n",

        "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(32)),\n",

        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",

        "])"

      ],

      "metadata": {

        "id": "xjsb55Jz_R9Y"

      },

      "execution_count": null,

      "outputs": []

    }

  ]

}

 

