{

  "nbformat": 4,

  "nbformat_minor": 0,

  "metadata": {

    "colab": {

      "provenance": []

    },

    "kernelspec": {

      "name": "python3",

      "display_name": "Python 3"

    },

    "language_info": {

      "name": "python"

    }

  },

  "cells": [

    {

      "cell_type": "markdown",

      "source": [

        "#Overview of NLP and its applications\n",

        "##Introduction to basic NLP tasks: tokenization, stemming, and lemmatization\n",

        "##Hands-on exercise: Tokenizing text using Python"

      ],

      "metadata": {

        "id": "tlmcZxgdeLLw"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "#Natural Language Processing (NLP) \n",

        "is a subfield of artificial intelligence and linguistics that focuses on the interaction between computers and human language. It involves the development of algorithms and techniques to enable computers to understand, interpret, and generate human language in a meaningful way. NLP has gained significant attention and has numerous applications across various domains.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "One of the primary applications of NLP is in language understanding and machine translation. NLP techniques enable machines to process and comprehend human language, making it possible to build systems that can automatically translate text from one language to another. This has paved the way for multilingual communication, cross-cultural collaboration, and global knowledge sharing.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "NLP is also widely used in information extraction and text mining. By analyzing and extracting structured information from unstructured text, NLP allows for the extraction of valuable insights and knowledge from large volumes of data. This is particularly useful in applications such as sentiment analysis, topic modeling, named entity recognition, and text summarization.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Another prominent application of NLP is in question answering and conversational agents. Natural language understanding and generation techniques enable the development of intelligent chatbots, virtual assistants, and voice-controlled systems. These systems can understand user queries, provide relevant responses, and engage in human-like conversations, enhancing user experiences and enabling seamless human-computer interactions.\n",

        "\n",

        "---\n",

        "\n",

        "NLP plays a crucial role in sentiment analysis and opinion mining, where it helps in determining the sentiment and subjective information expressed in text data. This has applications in areas such as social media monitoring, brand reputation management, market research, and customer feedback analysis.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "In addition, NLP is extensively used in the healthcare domain for clinical text analysis, medical records processing, and biomedical information extraction. It aids in extracting relevant medical information, identifying patterns, and assisting healthcare professionals in decision-making and research.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Furthermore, NLP finds applications in information retrieval, search engines, and document classification. By analyzing and understanding the content of documents and web pages, NLP enables efficient indexing, retrieval, and ranking of information, leading to improved search experiences and knowledge discovery.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "NLP also plays a vital role in natural language generation, text-to-speech synthesis, and language generation for chatbots, virtual assistants, and automated report generation. These capabilities enable machines to generate coherent and contextually appropriate human-like language, expanding the possibilities for automated content creation and personalized user experiences.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Overall, NLP is a rapidly evolving field with a wide range of applications that have the potential to revolutionize how we interact with computers, process information, and communicate with each other. Its versatility and impact span across various industries, including healthcare, education, e-commerce, customer service, finance, and many others."

      ],

      "metadata": {

        "id": "Z7edGAGtjTQO"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "#A sample text and exercises for tokenization, stemming, and lemmatization."

      ],

      "metadata": {

        "id": "aZo0p_e1Y5GQ"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\n",

        "\n",

        "https://www.nltk.org/"

      ],

      "metadata": {

        "id": "SRnZbqBlRup9"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Import necessary libraries\n",

        "import nltk\n",

        "from nltk.tokenize import word_tokenize\n",

        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",

        "from nltk import pos_tag\n",

        "from nltk.corpus import wordnet\n",

       "\n",

        "\n",

        "# Download required resources\n",

        "nltk.download('punkt')\n",

        "nltk.download('averaged_perceptron_tagger')\n",

        "nltk.download('wordnet')"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "uLvht_DIZE3A",

        "outputId": "c88233fb-267b-4906-ed3d-4e4c0d4fba36"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stderr",

          "text": [

            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",

            "[nltk_data]   Package punkt is already up-to-date!\n",

            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",

            "[nltk_data]     /root/nltk_data...\n",

            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",

            "[nltk_data]       date!\n",

            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",

            "[nltk_data]   Package wordnet is already up-to-date!\n"

          ]

        },

        {

          "output_type": "execute_result",

          "data": {

            "text/plain": [

              "True"

            ]

          },

          "metadata": {},

          "execution_count": 15

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Sample text for exercises\n",

        "text = \"I am enjoying this hands-on exercise. It is teaching me about NLP tasks like tokenization, stemming, and lemmatization.\"\n"

      ],

      "metadata": {

        "id": "H3sjoTSBZG1i"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "markdown",

      "source": [

        "Exercise 1: Tokenization\n",

        "\n",

        "Tokenize the given text using the word_tokenize() function from the nltk.tokenize module.\n",

        "Print the tokens.\n"

      ],

      "metadata": {

        "id": "rUCtbVsLiqcj"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Tokenization exercise\n",

        "print(\"Tokenization:\")\n",

        "tokens = word_tokenize(text)\n",

        "print(tokens)\n",

        "print()"

      ],

      "metadata": {

        "colab": {

         "base_uri": "https://localhost:8080/"

        },

        "id": "LwipNoG8i6bt",

        "outputId": "51321d63-bd66-46a3-d59b-f57f5141972a"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Tokenization:\n",

            "['I', 'am', 'enjoying', 'this', 'hands-on', 'exercise', '.', 'It', 'is', 'teaching', 'me', 'about', 'NLP', 'tasks', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",

            "\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "**Stemming** is a process of reducing words to their base or root form by removing suffixes or prefixes.\n",

        "It follows a simple and rule-based approach, chopping off the ends of words based on predefined rules to extract the stem.\n",

        "The resulting stems may not always be valid words or have a semantic meaning.\n",

        "Stemming is generally faster and computationally less expensive compared to lemmatization.\n",

        "\n",

        "Examples of stemming include transforming \"running\" to \"run,\" \"cats\" to \"cat,\" and \"jumped\" to \"jump.\"\n",

        "\n",

        "\n",

        "**Lemmatization** is the process of reducing words to their base or dictionary form (called lemmas), considering the word's morphological analysis and context.\n",

        "It takes into account the word's part of speech (POS) tag to ensure accurate normalization.\n",

        "Lemmatization uses lexical knowledge resources like WordNet or linguistic rules to transform words.\n",

        "The resulting lemmas are valid words with semantic meaning and can be used for analysis and comprehension.\n",

        "Lemmatization is generally slower and computationally more expensive compared to stemming.\n",

        "\n",

        "Examples of lemmatization include transforming \"running\" to \"run,\" \"cats\" to \"cat,\" and \"better\" to \"good.\"\n",

        "\n",

        "In summary, stemming is a simpler and more aggressive technique that removes prefixes and suffixes to extract the root form, while lemmatization considers the context and part of speech to normalize words to their base forms, resulting in valid words with semantic meaning. The choice between stemming and lemmatization depends on the specific NLP task and the level of normalization required.\n"

      ],

      "metadata": {

        "id": "0PBBlh1Mn68Q"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "Exercise 2: Stemming\n",

        "\n",

        "Initialize a PorterStemmer object from the nltk.stem module.\n",

        "Perform stemming on the tokens obtained from Exercise 1.\n",

        "Print the stemmed words."

      ],

     "metadata": {

        "id": "PhkVwvUCjbma"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# c exercise\n",

        "print(\"Stemming:\")\n",

        "stemmer = PorterStemmer()\n",

        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",

        "print(stemmed_words)\n",

        "print()"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "6_gOocuujdAr",

        "outputId": "f9a5ae79-d6a4-48f6-934c-51521b8849e6"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Stemming:\n",

            "['i', 'am', 'enjoy', 'thi', 'hands-on', 'exercis', '.', 'it', 'is', 'teach', 'me', 'about', 'nlp', 'task', 'like', 'token', ',', 'stem', ',', 'and', 'lemmat', '.']\n",

            "\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "Exercise 3: Lemmatization\n",

        "\n",

        "Initialize a WordNetLemmatizer object from the nltk.stem module.\n",

        "Perform lemmatization on the tokens obtained from Exercise 1.\n",

        "Print the lemmatized words."

      ],

      "metadata": {

        "id": "InpikRAhkL2h"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "# Lemmatization exercise\n",

        "print(\"Lemmatization:\")\n",

        "lemmatizer = WordNetLemmatizer()\n",

        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",

        "print(lemmatized_words)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "7O223sMIkPhL",

        "outputId": "590b82cd-3ed3-494c-b74a-b317e3bac95f"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Lemmatization:\n",

            "['I', 'am', 'enjoying', 'this', 'hands-on', 'exercise', '.', 'It', 'is', 'teaching', 'me', 'about', 'NLP', 'task', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"

          ]

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# POS tagging\n",

        "print(\"POS tagging:\")\n",

        "pos_tags = pos_tag(tokens)\n",

        "print(pos_tags)\n",

        "print()"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "GQL7O6SIk5Z3",

        "outputId": "cf9e24bd-4a52-49f0-dcf6-3041d3ce1443"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "POS tagging:\n",

            "[('I', 'PRP'), ('am', 'VBP'), ('enjoying', 'VBG'), ('this', 'DT'), ('hands-on', 'JJ'), ('exercise', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('teaching', 'VBG'), ('me', 'PRP'), ('about', 'IN'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('like', 'IN'), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n",

            "\n"

          ]

        }

      ]

    },

    {

      "cell_type": "code",

      "source": [

        "# Function to map POS tags to WordNet POS tags\n",

        "def get_wordnet_pos(tag):\n",

        "    if tag.startswith('J'):\n",

        "        return wordnet.ADJ\n",

        "    elif tag.startswith('V'):\n",

        "        return wordnet.VERB\n",

        "    elif tag.startswith('N'):\n",

        "        return wordnet.NOUN\n",

        "    elif tag.startswith('R'):\n",

        "        return wordnet.ADV\n",

        "    else:\n",

        "        return wordnet.NOUN"

      ],

      "metadata": {

        "id": "uelRBkrvl1hg"

      },

      "execution_count": null,

      "outputs": []

    },

    {

      "cell_type": "code",

      "source": [

        "\n",

        "# Lemmatization exercise\n",

        "print(\"Lemmatization:\")\n",

        "lemmatizer = WordNetLemmatizer()\n",

        "lemmatized_words = []\n",

        "for token, pos in pos_tags:\n",

        "    pos_tag = get_wordnet_pos(pos)  # Mapping POS tags to WordNet POS tags\n",

        "    lemmatized_word = lemmatizer.lemmatize(token, pos=pos_tag)\n",

        "    lemmatized_words.append(lemmatized_word)\n",

        "print(lemmatized_words)"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "5XHUza1-lxA3",

        "outputId": "e9920ce5-4d22-4cdb-e623-5f896699c126"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Lemmatization:\n",

            "['I', 'be', 'enjoy', 'this', 'hands-on', 'exercise', '.', 'It', 'be', 'teach', 'me', 'about', 'NLP', 'task', 'like', 'tokenization', ',', 'stem', ',', 'and', 'lemmatization', '.']\n"

          ]

        }

      ]

    },

    {

      "cell_type": "markdown",

      "source": [

        "Example of Lemmatization based on POS\n",

        "\n",

        "\"He is fishing.\"\n",

        "Lemmatized form: ['He', 'be', 'fish', '.']\n",

        "\n",

        "\"Fishing is a sport.\"\n",

        "Lemmatized form: ['Fishing', 'be', 'a', 'sport', '.']\n",

        "\n",

        "In the first sentence, \"fishing\" is lemmatized to \"fish\" to represent the base form of the verb. In the second sentence, \"fishing\" is lemmatized to \"Fishing\" as it is recognized as a noun and doesn't undergo any changes."

      ],

      "metadata": {

        "id": "8k5RTJrY2VGa"

      }

    },

    {

      "cell_type": "markdown",

      "source": [

        "#spaCy \n",

        "##https://spacy.io/\n",

        "is a powerful and widely used open-source library for natural language processing (NLP) in Python. It provides efficient and high-performance tools for various NLP tasks, making it popular among researchers, developers, and data scientists.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "One of spaCy's key capabilities is tokenization, which involves splitting text into individual words or tokens. It handles complex tokenization scenarios, such as contractions, hyphenated words, and punctuation, with excellent accuracy.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "spaCy also offers advanced linguistic annotations, including part-of-speech tagging, dependency parsing, named entity recognition (NER), and sentence segmentation. These annotations provide detailed linguistic information about the text, enabling deeper analysis and understanding of the language's structure and meaning.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Additionally, spaCy supports word vector representations, allowing users to access pre-trained word embeddings like GloVe or train custom models on their own data. These word vectors capture semantic relationships between words and are useful for tasks such as similarity calculations and text classification.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Another significant feature of spaCy is its efficient processing pipeline design, which allows for fast and scalable NLP workflows. It is designed to handle large volumes of text efficiently, making it suitable for real-world applications that require processing large datasets.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Furthermore, spaCy provides support for rule-based matching, allowing users to define custom patterns and extract information based on specific linguistic patterns or rules. This feature is beneficial for tasks such as extracting specific entities or specific linguistic patterns from the text.\n",

        "\n",

        "---\n",

        "\n",

        "\n",

        "\n",

        "Overall, spaCy is a comprehensive NLP library that combines performance, ease of use, and a wide range of capabilities. Its extensive features make it a go-to choice for tasks like text preprocessing, information extraction, text classification, and many other NLP applications."

      ],

      "metadata": {

        "id": "ggfm-xKUh7Qh"

      }

    },

    {

      "cell_type": "code",

      "source": [

        "import spacy\n",

        "\n",

        "# Load the English language model in spaCy\n",

        "nlp = spacy.load('en_core_web_sm')\n",

        "\n",

        "# Define a text for processing\n",

        "text = \"I am a runner running in a race. I have been running since morning.\"\n",

        "\n",

        "# Tokenization\n",

        "doc = nlp(text)\n",

        "tokens = [token.text for token in doc]\n",

        "\n",

        "print(\"Tokens:\", tokens)\n",

        "\n",

        "# Lemmatization with POS tagging\n",

        "lemmatized_tokens = [token.lemma_ if token.lemma_ != \"-PRON-\" else token.text for token in doc]\n",

        "\n",

        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",

        "\n"

      ],

      "metadata": {

        "colab": {

          "base_uri": "https://localhost:8080/"

        },

        "id": "vX_nsHVVgS8c",

        "outputId": "e7ebb012-89b4-449c-d3a5-101b403c7c5a"

      },

      "execution_count": null,

      "outputs": [

        {

          "output_type": "stream",

          "name": "stdout",

          "text": [

            "Tokens: ['I', 'am', 'a', 'runner', 'running', 'in', 'a', 'race', '.', 'I', 'have', 'been', 'running', 'since', 'morning', '.']\n",

            "Lemmatized Tokens: ['I', 'be', 'a', 'runner', 'run', 'in', 'a', 'race', '.', 'I', 'have', 'be', 'run', 'since', 'morning', '.']\n"

          ]

        }

      ]

    }

  ]

}

 

