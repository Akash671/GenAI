{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample vocabulary\n",
        "x = {'text', 'the', 'leader', 'prime', 'natural', 'language'}\n",
        "\n",
        "# Create the tokenizer and fit on texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# Number of unique words in the dictionary\n",
        "print(\"Number of unique words in dictionary=\", len(tokenizer.word_index))\n",
        "print(\"Dictionary is =\", tokenizer.word_index)\n",
        "\n",
        "# Function to load GloVe embeddings and create the embedding matrix\n",
        "def embedding_for_vocab(glove_file, word_index, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "# Path to GloVe embeddings file (ensure you have the file in the correct path)\n",
        "glove_file = '/content/sample_data/glove.6B.50d.txt'\n",
        "embedding_dim = 50\n",
        "\n",
        "# Create the embedding matrix for the vocabulary\n",
        "embedding_matrix_vocab = embedding_for_vocab(glove_file, tokenizer.word_index, embedding_dim)\n",
        "\n",
        "# Print the dense vector for the first word in the dictionary\n",
        "print(\"Dense vector for first word is =>\", embedding_matrix_vocab[1])\n",
        "\n",
        "# Function to calculate cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# Words to compare\n",
        "word1 = \"natural\"\n",
        "word2 = \"language\"\n",
        "\n",
        "# Check if words are in the tokenizer's word index\n",
        "if word1 in tokenizer.word_index and word2 in tokenizer.word_index:\n",
        "    vec1 = embedding_matrix_vocab[tokenizer.word_index[word1]]\n",
        "    vec2 = embedding_matrix_vocab[tokenizer.word_index[word2]]\n",
        "\n",
        "    # Calculate and print cosine similarity\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "else:\n",
        "    print(f\"One or both words are not in the vocabulary: '{word1}', '{word2}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waDrUfHgmNOR",
        "outputId": "8dfa2d90-24e5-4319-8b93-566593acd655"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 6\n",
            "Dictionary is = {'language': 1, 'natural': 2, 'the': 3, 'leader': 4, 'text': 5, 'prime': 6}\n",
            "Dense vector for first word is => [-5.79900026e-01 -1.10100001e-01 -1.15569997e+00 -2.99059995e-03\n",
            " -2.06129998e-01  4.52890009e-01 -1.66710004e-01 -1.03820002e+00\n",
            " -9.92410004e-01  3.98840010e-01  5.92299998e-01  2.29900002e-01\n",
            "  1.52129996e+00 -1.77640006e-01 -2.97259986e-01 -3.92349988e-01\n",
            " -7.84709990e-01  1.55939996e-01  6.90769970e-01  5.95369995e-01\n",
            " -4.43399996e-01  5.35139978e-01  3.28530014e-01  1.24370003e+00\n",
            "  1.29719996e+00 -1.38779998e+00 -1.09249997e+00 -4.09249991e-01\n",
            " -5.69710016e-01 -3.46560001e-01  3.71630001e+00 -1.04890001e+00\n",
            " -4.67079997e-01 -4.47389990e-01  6.22999994e-03  1.96490008e-02\n",
            " -4.01609987e-01 -6.29130006e-01 -8.25060010e-01  4.55909997e-01\n",
            "  8.26259971e-01  5.70909977e-01  2.11989999e-01  4.68650013e-01\n",
            " -6.00269973e-01  2.99199998e-01  6.79440022e-01  1.42379999e+00\n",
            " -3.21520008e-02 -1.26029998e-01]\n",
            "Cosine similarity between 'natural' and 'language': 0.45544978516767387\n"
          ]
        }
      ]
    }
  ]
}