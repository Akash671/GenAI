{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3DdiLtCNjC9",
        "outputId": "b8adb471-3fe3-4ec0-e5f3-1394fd18f3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Embedding for the word 'king':\n",
            "[ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained GloVe model (50-dimensional vectors)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word = \"king\"\n",
        "if word in model.key_to_index:\n",
        "    vector = model[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFQrYAA_PuHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# List all available models\n",
        "print(api.info()['models'].keys())\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model = api.load(\"word2vec-ruscorpora-300\")\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word = \"king\"  # \"king\" in Russian\n",
        "if word in model.key_to_index:\n",
        "    vector = model[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9CKvr_UPtpU",
        "outputId": "3ea37ab8-1bfc-48bb-a0b6-fb8601dc7577"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n",
            "The word 'king' is not in the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Get the vector for a word\n",
        "word = \"хороший\"  # \"king\" in Russian\n",
        "if word in model.key_to_index:\n",
        "    vector = model[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDGdo_KnQGuj",
        "outputId": "0060ea96-faad-4def-803c-28ba834d959f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'хороший' is not in the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we do not have smaller word2vec model in english, so we are\n",
        "#using a russian corpus, if we want a smaller model, we have\n",
        "#build it ourself like the one below"
      ],
      "metadata": {
        "id": "2QBPY9mXYfvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download the text8 corpus\n",
        "dataset = api.load('text8')  # Download the dataset\n",
        "\n",
        "# Preprocess the data\n",
        "sentences = []\n",
        "for line in dataset: # Iterate over the dataset, assuming it yields lists of tokens\n",
        "    sentence = ' '.join(line) # Join the tokens into a single string\n",
        "    sentences.append(utils.simple_preprocess(sentence)) # Preprocess the string\n",
        "\n",
        "# Train a smaller Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save('small_word2vec.model')\n",
        "\n",
        "# Load the saved model\n",
        "model = Word2Vec.load('small_word2vec.model')\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word = \"king\"\n",
        "if word in model.wv.key_to_index:\n",
        "    vector = model.wv[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AybjcSalXusE",
        "outputId": "3c0901ae-80cb-4f2d-d459-9609c45ff2e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for the word 'king':\n",
            "[ 3.6711376  -4.2833376   0.10618623  1.8498945  -1.8705447   1.5235114\n",
            "  1.950804    4.334738    1.6799122   0.6190093   1.1914827   3.6761503\n",
            "  0.37023193  4.24854     1.3171865   1.5447038   0.36289498 -0.8879646\n",
            "  0.8958131   0.672936   -1.8033546   2.6980915   2.7762985   2.4161088\n",
            " -1.3112208   2.229206    1.9919409   2.9710124   0.28833848 -0.9886999\n",
            " -3.893156    3.909353    2.7143004   2.6972888   2.278809   -1.7031083\n",
            " -0.9048188   1.1234881   3.0973113  -0.3575954   1.344546    5.0832644\n",
            " -0.1292342  -0.23958501  1.5862821   0.18369952  0.97416794 -1.5866162\n",
            " -2.8189178   2.1366262 ]\n"
          ]
        }
      ]
    }
  ]
}