{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download the text8 dataset if it's not already present\n",
        "dataset = api.load('text8')\n",
        "\n",
        "# Preprocess the data\n",
        "sentences = [' '.join(line) for line in dataset]  # Join words into a single string\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the pre-trained model\n",
        "model.save('pretrained_word2vec.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u90ALJ6TBcBn",
        "outputId": "2f697902-cf9d-4703-a5d6-25b6c2b0c1c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model = Word2Vec.load('pretrained_word2vec.model')\n",
        "\n",
        "# Your own small corpus\n",
        "corpus = [\n",
        "    ['this', 'is', 'a', 'new', 'sentence'],\n",
        "    ['another', 'sentence', 'for', 'fine', 'tuning'],\n",
        "    # Add more sentences as needed\n",
        "]\n",
        "\n",
        "# Fine-tune the model with your corpus\n",
        "model.build_vocab(corpus, update=True)\n",
        "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('fine_tuned_word2vec.model')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlWiER82BESX",
        "outputId": "29576dd7-5f1f-41ad-af34-55b4ca04fe29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = Word2Vec.load('fine_tuned_word2vec.model')\n"
      ],
      "metadata": {
        "id": "zkTN9DSo_2P2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the word you want to find the embedding for\n",
        "word = 'sentence'\n",
        "\n",
        "# Check if the word is in the vocabulary\n",
        "if word in model.wv.key_to_index:\n",
        "    # Get the embedding vector\n",
        "    embedding_vector = model.wv[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{embedding_vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sBg-47DDpcd",
        "outputId": "ee180a58-ad93-4bd3-9653-652962e90c90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for the word 'sentence':\n",
            "[-0.00566629 -0.00477596  0.00945929 -0.08547402  0.02343578 -0.02347684\n",
            " -0.00236269  0.02943614  0.05542883 -0.01529317  0.07136811 -0.04623141\n",
            "  0.00833025  0.0359754  -0.0320462   0.02336468 -0.03567854  0.00765077\n",
            "  0.01705562 -0.00471076 -0.01134801 -0.03133196 -0.04632283  0.0384473\n",
            " -0.04186878 -0.02172718  0.01871646 -0.05523174  0.02308986 -0.02891882\n",
            "  0.06347143 -0.02805673  0.01312747 -0.02779703  0.0038161   0.01587762\n",
            " -0.01598729  0.02406822  0.00406828  0.00980115 -0.03224684  0.01519683\n",
            "  0.0007463   0.01787868 -0.0218772  -0.06286106  0.01951836  0.00588414\n",
            " -0.05942003 -0.03267078 -0.02645404  0.01023962  0.06580212 -0.03750561\n",
            " -0.05182863 -0.01349448  0.00534535  0.0406668   0.0078284   0.04690418\n",
            "  0.00621407 -0.00780885 -0.00359881  0.0261384   0.02033752 -0.02861039\n",
            "  0.01583268 -0.02055888 -0.02547144  0.02937314 -0.07781123 -0.07988402\n",
            "  0.08487057  0.0646363  -0.01740045 -0.00070474 -0.02847248 -0.0186976\n",
            " -0.01065367  0.00158505 -0.00942499 -0.00856669  0.00520819  0.04607368\n",
            "  0.04862893 -0.01435899 -0.0333604   0.00608406 -0.03337945 -0.02435219\n",
            "  0.00982315 -0.03439971  0.04648447 -0.0081154   0.03086313 -0.00415331\n",
            "  0.02973126 -0.04828698  0.02563303 -0.02970353]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the text8 corpus\n",
        "dataset = api.load(\"text8\")\n",
        "\n",
        "# Build the Word2Vec model\n",
        "model = Word2Vec(dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save('text8_word2vec.model')\n",
        "\n",
        "# Load the saved model\n",
        "model = Word2Vec.load('text8_word2vec.model')\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word = \"king\"\n",
        "if word in model.wv.key_to_index:\n",
        "    vector = model.wv[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Fl_2yWMADc",
        "outputId": "014ea544-86dd-48ad-f99e-1d754c417f52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "Embedding for the word 'king':\n",
            "[-2.324854    1.0537138   3.5300672  -0.27410188  1.112171    4.034314\n",
            "  0.7149382   1.4151459  -3.6784499   0.47274044  0.5830803  -0.86555445\n",
            "  0.7004507   4.107307   -0.42135254 -1.1044253   1.1117389  -0.98056406\n",
            "  3.155394   -2.2107787  -0.6895847   3.6296952  -1.5270089   2.374229\n",
            "  2.7847693   0.449584    1.3031222   0.9952793  -1.8932054  -1.2520372\n",
            "  0.4231173   0.770195   -1.8840412   2.2435627   2.595141    2.1350915\n",
            "  1.9264061   1.3524189   1.4411801  -1.0919354  -0.5362063   1.2264758\n",
            "  0.24570417 -0.49052998 -1.8595148   1.5203513  -0.42497927 -1.6706431\n",
            "  2.9222326  -2.1031542   3.4698274   1.2972537  -2.845163    3.759958\n",
            " -0.06765749 -0.9252918  -0.4299855   1.1123027   1.1844947  -2.5237405\n",
            "  2.2416208   0.4678127   0.9740288  -0.94552153  1.1336007  -1.4928662\n",
            " -0.37538657  0.75676554  1.1662623  -2.6315148   1.3468332  -0.9357353\n",
            " -1.2409922  -2.510408    2.1008666  -1.4673597   1.368788    0.12870081\n",
            " -3.1238806   0.40992755 -1.0883881  -4.5083766   1.7417073   2.820125\n",
            " -0.92091614  1.0540814   1.9335409  -0.63359535 -0.21943857  2.2670166\n",
            " -2.2197475   2.726509   -0.8567789  -1.0307148   0.4677315  -0.43610972\n",
            "  0.19463594  1.3123329  -0.40305802  0.2130735 ]\n"
          ]
        }
      ]
    }
  ]
}